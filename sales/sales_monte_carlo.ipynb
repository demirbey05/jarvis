{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import SalesEnv\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SalesEnv(50,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMCAgent:\n",
    "\n",
    "    def __init__(self, env, gamma=1):\n",
    "        self.env = env\n",
    "        self.q_table = np.random.random((len(env.state_space), len(env.action_space)))\n",
    "        self.gamma = gamma\n",
    "        max = np.argmax(self.q_table, axis=1)\n",
    "        self.pi = np.zeros((len(env.state_space),len(env.action_space)))\n",
    "        for i in range(len(env.state_space)):\n",
    "            self.pi[i,max[i]] = 1\n",
    "        # Generate random positive numbers and normalize to create a valid probability distribution\n",
    "        self.b = np.random.uniform(0.1, 1.0, (len(env.state_space), len(env.action_space)))\n",
    "        self.b = self.b / self.b.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def generate_episode(self):\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        while True:\n",
    "            state_index = self.env.state_space.index(state)\n",
    "            action = np.random.choice(self.env.action_space, p=self.b[state_index])\n",
    "            next_state, reward, done = self.env.step(action)  # Unpack the result directly\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state.copy()  # Make sure to create a copy of the state\n",
    "            if done:\n",
    "                break\n",
    "              # Use the new state directly\n",
    "        return episode\n",
    "\n",
    "\n",
    "    def train(self,num_episodes = 1000000):\n",
    "        c = np.zeros((len(self.env.state_space), len(self.env.action_space)))\n",
    "        for i in tqdm.tqdm(range(num_episodes)):\n",
    "            episode = self.generate_episode()\n",
    "            g = 0\n",
    "            w = 1\n",
    "            for state, action, reward in reversed(episode):\n",
    "                # Get state and action index for the current state\n",
    "                state_index = self.env.state_space.index(state)\n",
    "                action_index = self.env.action_space.index(action)\n",
    "                g = self.gamma * g + reward\n",
    "                c[state_index, action_index] += w\n",
    "                self.q_table[state_index, action_index] += ( w / c[state_index, action_index]) * (g - self.q_table[state_index, action_index])\n",
    "\n",
    "                # Update policy\n",
    "                max = np.argmax(self.q_table[state_index])\n",
    "                self.pi[state_index,:] = np.zeros(len(self.env.action_space))\n",
    "                self.pi[state_index,max] = 1\n",
    "\n",
    "                if action != np.argmax(self.pi[state]):\n",
    "                    break\n",
    "                w = w / self.b[state_index, action_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [03:58<00:00, 4197.61it/s]\n"
     ]
    }
   ],
   "source": [
    "agent = OffPolicyMCAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.pi[env.state_space.index([30,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
